behaviors:
  SuperTank:
    trainer_type: ppo
    hyperparameters:
      batch_size: 1024                # Reduce el tamaño de lote para acelerar el aprendizaje inicial
      buffer_size: 20480              # Reduce el tamaño del buffer para una retroalimentación más rápida
      learning_rate: 3.0e-4           # Aumenta ligeramente la tasa de aprendizaje para un progreso más rápido
      beta: 1.0e-3                    # Ajusta la regularización para evitar un sobreajuste temprano
      epsilon: 0.2                    # Aumenta epsilon para permitir más exploración al inicio
      lambd: 0.93                     # Reduce ligeramente el factor de ventaja generalizada para un entorno más dinámico
      num_epoch: 3                    # Reduce las épocas para que el entrenamiento sea más rápido
      learning_rate_schedule: constant # Mantén la tasa de aprendizaje constante en etapas iniciales
      beta_schedule: constant
      epsilon_schedule: linear        # Mantén epsilon decreciendo para que el agente explore menos con el tiempo
    network_settings:
      normalize: true                 # Normalizar las observaciones es crucial
      hidden_units: 128               # Reduce las neuronas para evitar una red sobreajustada al principio
      num_layers: 2                   # Reduce el número de capas para hacer la red más ligera
    reward_signals:
      extrinsic:
        gamma: 0.99                   # Mantén un descuento de recompensa a largo plazo
        strength: 1.0                 # La señal extrínseca sigue siendo importante
    max_steps: 500000                 # Reduce los pasos máximos para obtener resultados iniciales más rápidos
    time_horizon: 64                  # Reduce el horizonte temporal para una retroalimentación más rápida
    summary_freq: 5000                # Aumenta la frecuencia de resúmenes para obtener más datos de rendimiento

