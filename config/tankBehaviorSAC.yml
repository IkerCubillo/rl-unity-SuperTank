behaviors:
  SuperTank:
    trainer_type: sac
    hyperparameters:
      batch_size: 256                # Tamaño de lote reducido para mejorar la estabilidad
      buffer_size: 20480            # Mayor tamaño de buffer para mejor aprendizaje a largo plazo
      learning_rate: 3.0e-4          # Tasa de aprendizaje estable y efectiva para SAC
      tau: 0.005                     # Tasa de actualización suave para las redes de valor
      reward_signal_steps_per_update: 1  # Actualizaciones más frecuentes para señales de recompensa
      init_entcoef: 0.1              # Coeficiente de entropía inicial para promover la exploración
      learning_rate_schedule: linear # Reduce la tasa de aprendizaje progresivamente
    network_settings:
      normalize: true                # Normaliza las observaciones para mejorar la estabilidad
      hidden_units: 128              # Más neuronas para manejar la complejidad del entorno
      num_layers: 2                  # Red más profunda para capturar mejor las interacciones del entorno
      vis_encode_type: simple        # Codificación simple para observaciones visuales (si aplica)
    reward_signals:
      extrinsic:
        gamma: 0.99                  # Descuento de recompensa para valorar recompensas a largo plazo
        strength: 1.0                # Peso de la señal extrínseca
    max_steps: 2000000               # Más pasos de entrenamiento para un aprendizaje profundo
    time_horizon: 512                # Horizonte temporal para capturar secuencias más largas de acciones
    summary_freq: 10000              # Resúmenes menos frecuentes para reducir el coste computacional

